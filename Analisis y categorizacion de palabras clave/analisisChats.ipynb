{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hago una limpieza de la tabla de interacciones entrantes  \n",
    "## Esta tiene unicamente mensajes de clientes, previamente filtre todos los mensajes automaticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_interaccionesEntrantes = 'interaccionesEntClean_noAuto.csv'  # Reemplaza esto con la ruta de tu archivo\n",
    "df_chats = pd.read_csv(db_interaccionesEntrantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chats['mensaje'] = df_chats['mensaje'].astype(str).fillna('')\n",
    "\n",
    "# Contar el número de palabras en cada fila\n",
    "df_chats['word_count'] = df_chats['mensaje'].str.split().apply(len)\n",
    "\n",
    "# Filtrar filas que tienen más de 3 palabras\n",
    "df_filtered = df_chats[df_chats['word_count'] > 2]\n",
    "\n",
    "\n",
    "df_filtered.to_excel('primer_filtro.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hago un impasse para averiguar cual es la incidencia de ciertas palabras clave dentro de la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_geografica = ['zona', 'ambientes', 'devoto', 'caba', 'departamentos', 'departamento', 'proyecto', 'caseros', 'donde', \n",
    "                    'belgrano', 'nuñez', 'capital', 'visita', 'dirección', 'casa', 'villa', 'ramos', 'recoleta', 'norte', 'lugar']\n",
    "\n",
    "lista_scheduling = ['mañana', 'día', 'momento', 'semana', 'tiempo', 'lunes', 'horario', 'hora', 'viernes', 'sábado', 'cita', \n",
    "                    'reunión', 'horarios', 'fecha', 'martes', 'jueves', 'coordinar', 'vacaciones', 'miércoles', 'mes']\n",
    "\n",
    "lista_pagos = ['cuotas', 'anticipo', 'mil', 'entrega', 'financiación', 'precio', 'pozo', 'precios', 'pago', 'pesos', 'cuota', \n",
    "               'años', 'financiamiento', 'usd', 'presupuesto', 'interés', 'pagos', 'costo', 'financiar', 'crédito']\n",
    "\n",
    "lista_info = ['información', 'info', 'contacto', 'anuncio', 'consulta', 'mensaje', 'oficina', 'proyectos', 'dólares', 'instagram', \n",
    "              'publicación', 'spazios', 'respuesta', 'datos', 'pregunta', 'empresa', 'detalles', 'asistencia', 'futuro', 'servicios']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Crear un diccionario para almacenar las listas y sus nombres\n",
    "listas_palabras = {\n",
    "    'df_geografica': lista_geografica,\n",
    "    'df_schedule': lista_scheduling,\n",
    "    'df_pagos': lista_pagos,\n",
    "    'df_info': lista_info\n",
    "    }\n",
    "\n",
    "# Iterar sobre las listas y crear un DataFrame filtrado para cada una\n",
    "dataframes_filtrados = {}\n",
    "\n",
    "# Filtrar las filas que contienen alguna de las palabras clave\n",
    "for nombre_df, lista_palabras in listas_palabras.items():\n",
    "    # Crear la expresión regular para la lista actual\n",
    "    regex = r'\\b(?:' + '|'.join(lista_palabras) + r')\\b'\n",
    "    \n",
    "    # Filtrar el DataFrame original según la lista actual\n",
    "    df_filtrado = df_filtered[df_filtered['mensaje'].str.contains(regex, regex=True, case=False, na=False)]\n",
    "    \n",
    "    # Guardar el DataFrame filtrado en el diccionario\n",
    "    dataframes_filtrados[nombre_df] = df_filtrado\n",
    "    \n",
    "    # (Opcional) Guardar cada DataFrame en un archivo CSV\n",
    "    df_filtrado.to_csv(f'{nombre_df}.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filtro_automatico_pagos = pd.read_csv('df_pagos.csv')\n",
    "filtro_automatico_info = pd.read_csv('df_info.csv')\n",
    "filtro_automatico_schedule = pd.read_csv('df_schedule.csv')\n",
    "filtro_automatico_geografia = pd.read_csv('df_geografica.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n",
      "1.13.1\n",
      "1.5.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "print(np.__version__)\n",
    "print(scipy.__version__)\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtro_automatico_pagos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Sample data\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mfiltro_automatico_pagos\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmensaje\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Convert the data to TF-IDF vectors\u001b[39;00m\n\u001b[0;32m      9\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filtro_automatico_pagos' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = filtro_automatico_pagos['mensaje']\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data)\n",
    "\n",
    "\n",
    "n_clusters = 12 \n",
    "kmeans = KMeans(n_clusters=n_clusters,random_state=44)\n",
    "kmeans.fit(X)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Item': data, 'Cluster': kmeans.labels_})\n",
    "\n",
    "\n",
    "df.to_csv('Cluster_pagos.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "['no', 'información', 'la', 'por', 'gracias']\n",
      "Topic 2:\n",
      "['si', 'información', 'info', 'me', 'por']\n",
      "Topic 3:\n",
      "['el', 'la', 'es', 'en', 'de']\n",
      "Topic 4:\n",
      "['no', 'que', 'de', 'la', 'consulta']\n",
      "Topic 5:\n",
      "['que', 'de', 'oficina', 'en', 'la']\n",
      "Topic 6:\n",
      "['para', 'me', 'que', 'más', 'información']\n",
      "Topic 7:\n",
      "['pero', 'de', 'me', 'que', 'no']\n",
      "Topic 8:\n",
      "['en', 'de', 'por', 'mensaje', 'tu']\n",
      "Topic 9:\n",
      "['los', 'hola', 'que', 'día', 'buen']\n",
      "Topic 10:\n",
      "['la', 'confirmo', 'asistencia', 'tenes', 'si']\n",
      "Topic 11:\n",
      "['hola', 'información', 'los', 'en', 'de']\n",
      "Topic 12:\n",
      "['te', 'me', 'gracias', 'en', 'contacto']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[8.33337201e-02, 8.33339166e-02, 8.33333333e-02, ...,\n",
       "        8.33333333e-02, 8.33333333e-02, 8.33333333e-02],\n",
       "       [8.33339171e-02, 8.33341064e-02, 8.33333333e-02, ...,\n",
       "        8.33333333e-02, 8.33333333e-02, 8.33333333e-02],\n",
       "       [8.33378591e-02, 9.59860807e+01, 8.33400051e-02, ...,\n",
       "        8.33333333e-02, 8.33333333e-02, 8.33333333e-02],\n",
       "       ...,\n",
       "       [1.51696634e+00, 8.33344784e-02, 8.33333333e-02, ...,\n",
       "        8.33333333e-02, 8.33333333e-02, 8.33333333e-02],\n",
       "       [8.33338866e-02, 8.33342293e-02, 8.33356682e-02, ...,\n",
       "        8.33333333e-02, 8.33333333e-02, 8.33333333e-02],\n",
       "       [8.33337791e-02, 8.33335146e-02, 8.33333333e-02, ...,\n",
       "        8.33333333e-02, 8.33333333e-02, 8.33333333e-02]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=12, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "words = vectorizer.get_feature_names_out()\n",
    "for index, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {index + 1}:\")\n",
    "    print([words[i] for i in topic.argsort()[-5:]])\n",
    "lda.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTRO DIRILECH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.053*\"de\" + 0.030*\"y\" + 0.023*\"no\" + 0.022*\"el\" + 0.019*\"es\" + 0.016*\"me\" '\n",
      "  '+ 0.016*\"que\" + 0.015*\"la\" + 0.015*\"en\" + 0.014*\"a\"'),\n",
      " (1,\n",
      "  '0.042*\"en\" + 0.030*\"y\" + 0.029*\"de\" + 0.023*\"que\" + 0.018*\"el\" + 0.016*\"la\" '\n",
      "  '+ 0.014*\"cuotas\" + 0.013*\"me\" + 0.012*\"los\" + 0.012*\"saber\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "documents = filtro_automatico_pagos['mensaje']\n",
    "\n",
    "tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "\n",
    "dictionary = corpora.Dictionary(tokenized_docs)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary)\n",
    "\n",
    "pprint(lda_model.print_topics())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
